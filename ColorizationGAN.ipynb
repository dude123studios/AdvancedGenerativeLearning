{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ColorizationGAN",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPgIrNHjhKnTMUM7ZSgMvwY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dude123studios/GANS/blob/main/ColorizationGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fVwvbCoWwlRY"
      },
      "source": [
        "mport tensorflow as tf\r\n",
        "import numpy as np\r\n",
        "import cv2 \r\n",
        "from tensorflow.keras.losses import mean_squared_error\r\n",
        "from tensorflow.keras.losses import mean_absolute_error\r\n",
        "import random \r\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7cZ_fSji2tHZ",
        "outputId": "8dfec076-5781-480f-dbd1-6484aaff4193"
      },
      "source": [
        "(data,_),(test,_) = tf.keras.datasets.cifar10.load_data()\r\n",
        "print(data.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 2s 0us/step\n",
            "(50000, 32, 32, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yWlgwDtC3zYG",
        "outputId": "9143ff5f-1c10-4a0f-9f0a-c90ec08d32bf"
      },
      "source": [
        "def grayConversion(image):\r\n",
        "    #                   blue                    green                   red\r\n",
        "    grayValue = 0.07 * image[:,:,2] + 0.72 * image[:,:,1] + 0.21 * image[:,:,0]\r\n",
        "    gray_img = grayValue.astype(np.uint8)\r\n",
        "    return gray_img\r\n",
        "new_arr = []\r\n",
        "for img in data:\r\n",
        "  img = grayConversion(img)\r\n",
        "  new_arr.append(img)\r\n",
        "gray = np.asarray(new_arr)\r\n",
        "gray = gray.astype('float32')\r\n",
        "gray = gray.reshape((50000,32,32,1))\r\n",
        "print(gray.shape)\r\n",
        "\r\n",
        "new_arr = []\r\n",
        "for img in test:\r\n",
        "  img = grayConversion(img)\r\n",
        "  new_arr.append(img)\r\n",
        "gray_test = np.asarray(new_arr)\r\n",
        "gray_test = gray_test.astype('float32')\r\n",
        "gray_test = gray_test.reshape((10000,32,32,1))\r\n",
        "print(gray_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(50000, 32, 32, 1)\n",
            "(10000, 32, 32, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5CEjzt61jsh"
      },
      "source": [
        "def downsample(filters, size=3, apply_batchnorm=True):\r\n",
        "    initializer = tf.random_normal_initializer(0., 0.02)\r\n",
        "\r\n",
        "    result = tf.keras.Sequential()\r\n",
        "    result.add(\r\n",
        "      tf.keras.layers.Conv2D(filters, size, strides=2, padding='same',\r\n",
        "                             kernel_initializer=initializer, use_bias=False))\r\n",
        "    if apply_batchnorm:\r\n",
        "        result.add(tf.keras.layers.BatchNormalization())\r\n",
        "    result.add(tf.keras.layers.LeakyReLU())\r\n",
        "\r\n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKx4V4PV126m"
      },
      "source": [
        "def upsample(filters, size=3, apply_dropout=False):\r\n",
        "    initializer = tf.random_normal_initializer(0., 0.02)\r\n",
        "\r\n",
        "    result = tf.keras.Sequential()\r\n",
        "    result.add(\r\n",
        "    tf.keras.layers.Conv2DTranspose(filters, size, strides=2,\r\n",
        "                                    padding='same',\r\n",
        "                                    kernel_initializer=initializer,\r\n",
        "                                    use_bias=False))\r\n",
        "\r\n",
        "    result.add(tf.keras.layers.BatchNormalization())\r\n",
        "\r\n",
        "    if apply_dropout:\r\n",
        "        result.add(tf.keras.layers.Dropout(0.5))\r\n",
        "\r\n",
        "    result.add(tf.keras.layers.ReLU())\r\n",
        "\r\n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iN5PZ7On14gp"
      },
      "source": [
        "class ResnetIdentityBlock(tf.keras.Model):\r\n",
        "  def __init__(self, kernel_size, filters):\r\n",
        "    super(ResnetIdentityBlock, self).__init__(name='')\r\n",
        "    filters1, filters2, filters3 = filters\r\n",
        "\r\n",
        "    self.conv2a = tf.keras.layers.Conv2D(filters1, (1, 1))\r\n",
        "    self.bn2a = tf.keras.layers.BatchNormalization()\r\n",
        "\r\n",
        "    self.conv2b = tf.keras.layers.Conv2D(filters2, kernel_size, padding='same')\r\n",
        "    self.bn2b = tf.keras.layers.BatchNormalization()\r\n",
        "\r\n",
        "    self.conv2c = tf.keras.layers.Conv2D(filters3, (1, 1))\r\n",
        "    self.bn2c = tf.keras.layers.BatchNormalization()\r\n",
        "\r\n",
        "  def call(self, input_tensor, training=False):\r\n",
        "    x = self.conv2a(input_tensor)\r\n",
        "    x = self.bn2a(x, training=training)\r\n",
        "    x = tf.nn.relu(x)\r\n",
        "\r\n",
        "    x = self.conv2b(x)\r\n",
        "    x = self.bn2b(x, training=training)\r\n",
        "    x = tf.nn.relu(x)\r\n",
        "\r\n",
        "    x = self.conv2c(x)\r\n",
        "    x = self.bn2c(x, training=training)\r\n",
        "\r\n",
        "    x += input_tensor\r\n",
        "    return tf.nn.relu(x)\r\n",
        "\r\n",
        "    \r\n",
        "block1 = ResnetIdentityBlock(3, [128, 128, 128])\r\n",
        "block2 = ResnetIdentityBlock(3, [128, 128,128])\r\n",
        "block3 = ResnetIdentityBlock(3, [128, 128, 128])\r\n",
        "\r\n",
        "\r\n",
        "resnet = [block1, block2, block3]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jxWX-DcB17NL"
      },
      "source": [
        "\r\n",
        "def Generator():\r\n",
        "    down_stack = [\r\n",
        "        downsample(32, 3, apply_batchnorm=False), \r\n",
        "        downsample(64, 3),\r\n",
        "        downsample(128, 3), \r\n",
        "    ]\r\n",
        "\r\n",
        "    up_stack = [\r\n",
        "        upsample(64, 3),\r\n",
        "        upsample(32, 3), \r\n",
        "    ]\r\n",
        "\r\n",
        "    initializer = tf.random_normal_initializer(0., 0.02)\r\n",
        "    last = tf.keras.layers.Conv2DTranspose(3, 3,\r\n",
        "                                         strides=2,\r\n",
        "                                         padding='same',\r\n",
        "                                         kernel_initializer=initializer,\r\n",
        "                                         activation='sigmoid') \r\n",
        "\r\n",
        "\r\n",
        "    inputs = tf.keras.layers.Input(shape=[32, 32, 1])\r\n",
        "    x = inputs\r\n",
        "\r\n",
        "    # Downsampling through the model\r\n",
        "    skips = []\r\n",
        "    for down in down_stack:\r\n",
        "        x = down(x)\r\n",
        "        skips.append(x)\r\n",
        "        \r\n",
        "    for block in resnet:\r\n",
        "        x = block(x)\r\n",
        "\r\n",
        "    skips = reversed(skips[:-1])\r\n",
        "\r\n",
        "    # Upsampling and establishing the skip connections\r\n",
        "    for up, skip in zip(up_stack, skips):\r\n",
        "        concat = tf.keras.layers.Concatenate()\r\n",
        "        x = up(x)\r\n",
        "        x = concat([x, skip])\r\n",
        "\r\n",
        "    x = last(x)\r\n",
        "\r\n",
        "    return tf.keras.Model(inputs=inputs, outputs=x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bdHoRGQR1-GU"
      },
      "source": [
        "def Discriminator():\r\n",
        "    inputs = tf.keras.layers.Input(shape=[None,None,3])\r\n",
        "    x = inputs\r\n",
        "    g_filter = 32\r\n",
        "    \r\n",
        "    down_stack = [\r\n",
        "        downsample(g_filter),\r\n",
        "        downsample(g_filter * 2),\r\n",
        "        downsample(g_filter * 4),\r\n",
        "    ]\r\n",
        "    \r\n",
        "    for down in down_stack:\r\n",
        "        x = down(x)\r\n",
        "\r\n",
        "    last = tf.keras.layers.Conv2D(1, 4, strides=1, padding='same') \r\n",
        "    x = last(x)\r\n",
        "\r\n",
        "    return tf.keras.Model(inputs=inputs, outputs=x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IchdjNfeQxLf"
      },
      "source": [
        "generator = Generator()\r\n",
        "discriminator = Discriminator()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y4MMK0pRj81f"
      },
      "source": [
        "loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\r\n",
        "optimizer = tf.keras.optimizers.Adam(beta_1=0.7)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zbt62qI2WVwR"
      },
      "source": [
        "@tf.function\r\n",
        "def grayConversionTensors(imgs):  \r\n",
        "  return tf.math.add(tf.math.add(tf.slice(imgs,[0,0,0,0],[-1,-1,-1,1])*0.21,tf.slice(imgs,[0,0,0,1],[-1,-1,-1,1])*0.72), 0.07 * tf.slice(imgs,[0,0,0,2],[-1,-1,-1,1]))\r\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OBQVGclQfsqT"
      },
      "source": [
        "@tf.function\r\n",
        "def train_batch(gray_imgs,normal_imgs):\r\n",
        "  with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\r\n",
        "    fake_imgs = generator(gray_imgs,training=True)\r\n",
        "    \r\n",
        "    logits_real = discriminator(normal_imgs,training=True)\r\n",
        "    logits_fake = discriminator(fake_imgs,training=True)\r\n",
        "\r\n",
        "    rl_loss = loss(tf.ones_like(logits_real),logits_real)\r\n",
        "    fk_loss = loss(tf.zeros_like(logits_fake),logits_fake)\r\n",
        "    disc_loss = rl_loss + fk_loss\r\n",
        "    \r\n",
        "    gray_color_imgs = grayConversionTensors(fake_imgs)\r\n",
        "\r\n",
        "    gen_loss = tf.math.reduce_sum([\r\n",
        "        tf.math.reduce_mean(loss(tf.ones_like(logits_fake),logits_fake)),\r\n",
        "        tf.math.reduce_mean(tf.square(gray_color_imgs-gray_imgs)),\r\n",
        "      ])\r\n",
        "    \r\n",
        "    grad = disc_tape.gradient(disc_loss,discriminator.trainable_variables)\r\n",
        "    optimizer.apply_gradients(zip(grad,discriminator.trainable_variables))\r\n",
        "    grad = gen_tape.gradient(gen_loss,generator.trainable_variables)\r\n",
        "    optimizer.apply_gradients(zip(grad,generator.trainable_variables))\r\n",
        "\r\n",
        "    return disc_loss, gen_loss\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5IwwJOHTraGO"
      },
      "source": [
        "\r\n",
        "def train(gray_imgs,real_imgs,test_gray_imgs,test_real_imgs,epochs=50,batch_size=10):\r\n",
        "  batches_per_epoch = int(gray_imgs.shape[0]/batch_size)\r\n",
        "  for epoch in range(epochs):\r\n",
        "    for batch in range(0,batches_per_epoch-1):\r\n",
        "      batch_imgs_x = gray_imgs[batch*batch_size:(batch+1)*batch_size]\r\n",
        "      batch_imgs_y = real_imgs[batch*batch_size:batch*batch_size+batch_size]\r\n",
        "      disc_loss, gen_loss = train_batch(batch_imgs_x,batch_imgs_y)\r\n",
        "      if batch == 0:\r\n",
        "          print(\"discriminator: \", disc_loss.numpy)\r\n",
        "          print(\"generator: {}\\n\".format(gen_loss.numpy))\r\n",
        "          '''\r\n",
        "          idx = random.randint(0,9999)\r\n",
        "          fig, axs = plt.subplots(3, 1, figsize=(10, 10), sharey=True, sharex=True)\r\n",
        "          gen_outputA = generator(test_gray_imgs[idx], training=False)\r\n",
        "          axs[0,0].imshow(test_gray_imgs[idx])\r\n",
        "          axs[0,0].set_title(\"Gray Image\")\r\n",
        "          axs[1,0].imshow(gen_outputA[0])\r\n",
        "          axs[1,0].set_title(\"Synthesized Image\")\r\n",
        "          axs[2,0].imshow(test_real_imgs[0])\r\n",
        "          axs[2,0].set_title(\"Real Image\")\r\n",
        "          plt.show()\r\n",
        "          '''\r\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GOfcdwi2iNVK"
      },
      "source": [
        "with tf.device('gpu:0'):\r\n",
        "  train(gray,data,gray_test,test)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}